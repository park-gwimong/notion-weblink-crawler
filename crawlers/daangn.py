# -*- coding: utf-8 -*-

import re
from datetime import datetime
from typing import List, Dict, Any
from html import unescape

import feedparser

from .base import BaseCrawler, Post


class DaangnCrawler(BaseCrawler):
    """ë‹¹ê·¼ë§ˆì¼“ ê¸°ìˆ  ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ (RSS í”¼ë“œ)"""

    name = "ë‹¹ê·¼"
    source_id = "daangn"
    base_url = "https://medium.com/daangn"
    feed_url = "https://medium.com/feed/daangn"

    def fetch(self) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œì—ì„œ ìµœì‹  ê¸€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"  ğŸŒ {self.name} RSS í”¼ë“œ ë¡œë”© ì¤‘...")
            feed = feedparser.parse(self.feed_url)

            if feed.bozo and not feed.entries:
                print(f"âŒ {self.name} RSS íŒŒì‹± ì‹¤íŒ¨: {feed.bozo_exception}")
                return []

            posts = []
            for entry in feed.entries[:self.max_posts]:
                post = self._parse_entry(entry)
                if post:
                    posts.append(post.to_dict())

            print(f"  âœ… {len(posts)}ê°œ ê¸€ íŒŒì‹± ì™„ë£Œ")
            return posts

        except Exception as e:
            print(f"âŒ {self.name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []

    def _parse_entry(self, entry) -> Post:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ Post ê°ì²´ ìƒì„±"""
        title = entry.get('title', '').strip()
        url = entry.get('link', '').strip()

        if not title or not url:
            return None

        # ìš”ì•½ ì¶”ì¶œ (HTML íƒœê·¸ ì œê±°)
        summary = self._extract_summary(entry)

        # ë‚ ì§œ íŒŒì‹±
        date = self._parse_date(entry)

        return Post(
            title=title,
            url=url,
            summary=summary,
            date=date,
            source=self.source_id,
        )

    def _extract_summary(self, entry) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ìš”ì•½ ì¶”ì¶œ"""
        # summary ë˜ëŠ” description í•„ë“œ ì‚¬ìš©
        raw = entry.get('summary', '') or entry.get('description', '')

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', raw)
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = unescape(text)
        # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()

        # ìµœëŒ€ 500ìë¡œ ì œí•œ
        if len(text) > 500:
            text = text[:497] + '...'

        return text

    def _parse_date(self, entry) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ë‚ ì§œ íŒŒì‹± (YYYY.MM.DD í˜•ì‹)"""
        # published_parsed ë˜ëŠ” updated_parsed ì‚¬ìš©
        time_struct = entry.get('published_parsed') or entry.get('updated_parsed')

        if time_struct:
            try:
                dt = datetime(*time_struct[:6])
                return dt.strftime('%Y.%m.%d')
            except Exception:
                pass

        # ë¬¸ìì—´ì—ì„œ íŒŒì‹± ì‹œë„
        date_str = entry.get('published', '') or entry.get('updated', '')
        if date_str:
            # RFC 2822 í˜•ì‹ íŒŒì‹± ì‹œë„
            try:
                from email.utils import parsedate_to_datetime
                dt = parsedate_to_datetime(date_str)
                return dt.strftime('%Y.%m.%d')
            except Exception:
                pass

        return datetime.now().strftime('%Y.%m.%d')

    def parse_posts(self, page) -> List[Post]:
        """RSS ê¸°ë°˜ì´ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„)"""
        return []


# í¸ì˜ë¥¼ ìœ„í•œ í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤
def fetch_daangn_posts():
    """ë‹¹ê·¼ë§ˆì¼“ ê¸°ìˆ  ë¸”ë¡œê·¸ì—ì„œ ìµœì‹  ê¸€ ê°€ì ¸ì˜¤ê¸°"""
    crawler = DaangnCrawler()
    return crawler.fetch()
